{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reliable-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compliant-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define run parmameters\n",
    "\n",
    "model_name = \"squeezenet\"\n",
    "\n",
    "transform_image_size = 224\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "mode = 0o777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pregnant-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data dict\n",
    "\n",
    "hand_type = 'dorsal'\n",
    "\n",
    "data_directory = \"./dataset_final/\" + hand_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "genuine-classic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Check device\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "alert-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase number of unique images by factor of 2 by horizontal flipping,\n",
    "# but cant vertically flip\n",
    "\n",
    "# Change colour channels from 0-255 to 0-1, numpy to tensors\n",
    "\n",
    "train_transformation = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(transform_image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406], # col = rgb channel\n",
    "        [0.229, 0.224, 0.225]) # row = mean deviation\n",
    "])\n",
    "\n",
    "valid_transformation = transforms.Compose([\n",
    "        transforms.Resize(transform_image_size),\n",
    "        transforms.CenterCrop(transform_image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406],\n",
    "            [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "opening-structure",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset_final/dorsal/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cd95303efc82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_transformation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_transformation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[0;32m--> 253\u001b[0;31m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001b[0m\u001b[1;32m    254\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    124\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    125\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mNo\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \"\"\"\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset_final/dorsal/train'"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_directory, 'train'), train_transformation)\n",
    "valid_dataset = datasets.ImageFolder(os.path.join(data_directory, 'valid'), valid_transformation)\n",
    "\n",
    "# Dataloader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True, # Shuffle is True so the model is not biased to some categories\n",
    "    num_workers=4 # How many sub processes to use for loading data into ram\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = pathlib.Path(data_directory + '/train')\n",
    "data_classes = sorted([item.name.split('/')[-1] for item in train_root.iterdir()])\n",
    "num_classes = len(data_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=True)\n",
    "        for parameter in model_ft.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg16_bn(pretrained=True)\n",
    "        for parameter in model_ft.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=True)\n",
    "        for parameter in model_ft.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_ft.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = model.parameters()\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "        \n",
    "optimiser = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc amount of training and validation images for each class\n",
    "\n",
    "train_male = len(glob.glob(data_directory + '/train/male/*.jpg'))\n",
    "train_female = len(glob.glob(data_directory + '/train/female/*.jpg'))\n",
    "train_count = train_male + train_female\n",
    "\n",
    "valid_male = len(glob.glob(data_directory + '/valid/male/*.jpg'))\n",
    "valid_female = len(glob.glob(data_directory + '/valid/female/*.jpg'))\n",
    "valid_count = valid_male + valid_female\n",
    "\n",
    "print('train male:', train_male, '\\ttrain female:', train_female, '\\ttrain count:', train_count)\n",
    "print('valid male:', valid_male, '\\tvalid female:', valid_female, '\\tvalid count:', valid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phase(phase_type, phase_count, phase_dataloader):\n",
    "    since = time.time()\n",
    "    phase_accuracy = 0.0\n",
    "    phase_loss = 0.0\n",
    "    # image_count = 0\n",
    "\n",
    "    for images, labels in phase_dataloader:\n",
    "        inputs = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimiser.zero_grad() # Zero out gradients when starting the training loop,\n",
    "                              # else gradient will point away from objective direction\n",
    "\n",
    "        with torch.set_grad_enabled(phase_type == 'train'):\n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "\n",
    "            if phase_type == 'train':\n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "\n",
    "        # Calc stats\n",
    "        phase_accuracy = phase_accuracy + (torch.sum(prediction == labels.data))\n",
    "        phase_loss = phase_loss + (loss.item() * images.size(0))\n",
    "\n",
    "        # Update image counter and elpased time\n",
    "        # time_elapsed = time.time() - since\n",
    "        # time_min = time_elapsed // 60\n",
    "        # time_sec = time_elapsed % 60\n",
    "        # print('\\r' + 'Image: ' + str(image_count), '\\tElpased Time:', '{:.0f}m {:.0f}s'.format(time_min, time_sec), end='')\n",
    "        # image_count+=1\n",
    "\n",
    "    epoch_accuracy = phase_accuracy.double() / phase_count\n",
    "    epoch_loss = phase_loss / phase_count\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    time_min = time_elapsed // 60\n",
    "    time_sec = time_elapsed % 60\n",
    "    \n",
    "    print('\\n{} \\tLoss: {:.4f} \\tAcc: {:.4f} \\tElpased Time: {:.0f}m {:.0f}s'.format(phase_type, epoch_loss, epoch_accuracy, time_min, time_sec ))\n",
    "    \n",
    "    return epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "def train(model, train_dataloader, valid_dataloader, optimiser, num_epochs):\n",
    "    best_acc = 0.0\n",
    "    epoch_accuracies = []\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print('Epoch', str(epoch + 1) + '/' + str(num_epochs))\n",
    "\n",
    "        # Train model phase\n",
    "        model.train()\n",
    "        run_phase('train', train_count, train_dataloader)\n",
    "\n",
    "        # Eval model phase\n",
    "        model.eval()\n",
    "        epoch_acc = run_phase('valid', valid_count, valid_dataloader)\n",
    "        \n",
    "        # Check performace\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        # Record performance for analysis\n",
    "        epoch_accuracies.append(epoch_acc)\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(best_acc)\n",
    "    \n",
    "    return best_model, epoch_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, epoch_accuracies = train(model, train_dataloader, valid_dataloader, optimiser, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(folder_path):\n",
    "    try: \n",
    "        os.makedirs(os.path.join(folder_path), mode)\n",
    "    except OSError as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = 'trained_models/'\n",
    "create_dir(models_dir)\n",
    "models_dir = os.path.join(models_dir, model_name)\n",
    "torch.save(model, models_dir + ('_' + hand_type + '_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [h.cpu().numpy() for h in epoch_accuracies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "\n",
    "plt.plot(range(1,num_epochs+1),history,label=\"Pretrained\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-shade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
